---
title: "Web Traffic Time Series Forecasting"
author: "Devanshi Verma"
date: "11/27/2020"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 5
---

### Overview

#### Description

We decided to work on one of the most burning time series problem of today’s day and era, “predicting web traffic”. We believe that this forecasting can help website servers a great deal in effectively handling outages. The technique we implemented can be extended to diverse applications in financial markets, weather forecasts, audio and video processing. Not just that, understanding your website’s traffic trajectory can open up business opportunities too!

#### Dataset
The dataset consists of 145k time series representing the number of daily page views of different Wikipedia articles, starting from July 1st, 2015 up until September 10th, 2017 (804 data points). The goal is to forecast the daily views between September 13th, 2017 and November 13th, 2017 (64 data points) for each article in the dataset.Traditional moving averages, ARIMA based techniques


Let's import the important libraries
```{r, message = FALSE, warning = FALSE}
# visualization
library('ggplot2') 

# data manipulation
library('dplyr')
library('readr')
library('imputeTS')


#time series
library('fpp')
library('forecast')
library('xts')
library('zoo')
library(prophet)
library('TSA')
```

### Data Preparation
A dataset like this would have a lot of interventions because the web traffic views are dependent on external factors and can spike up on one day and return to the same level in a while. So let's start with searching a nice stationary model wave where we can use our ARIMA methods, and then we can start working on interventions

```{r, message = FALSE, warning = FALSE}
# importing the dataset
train_1 <- read.csv("~/Desktop/UChicago/Quarters/03-Quarters/Data/TS/web-traffic-time-series-forecasting/train_1.csv", header = TRUE, row.names = 1,sep = ",",skip =0)
```

Let's have a look at different time series available

```{r, message = FALSE, warning = FALSE}
# importing the dataset
Google <- data.matrix(train_1[c("Google_zh.wikipedia.org_all-access_spider"),])
dimnames(Google)<-NULL
Google<-array(Google)
time_index <- seq(from = as.POSIXct("2015-07-01"), to = as.POSIXct("2016-12-31"), by = "day")
Google_ts <- xts(Google, order.by =time_index ,frequency = 365.25)
autoplot(Google_ts)
```

We can see that the data more or less appears stationary but we have a spike im 2015 December. This is a very common  pattern we can see when we have web traffic data since it's heavily influenced by external factors. It needs to be researched further why something a topic has spike and whether it's an intervention or not. In order to check for interventions we first need to model before and after the intervention point and analyze whether it should be an intervention or not. We have a separate notebooks as to how to deal with these interventions. In this notebook we would be dealing only with a data that appears more or less stationary. Let's have a look at other cases.


Let's look at another dataset. 

```{r, message = FALSE, warning = FALSE}
# importing the dataset
ASCII <- data.matrix(train_1[c("ASCII_zh.wikipedia.org_all-access_spider"),])
dimnames(ASCII)<-NULL
ASCII<-array(ASCII)
plot(ASCII,type='l')
```
In this case we have an intervention again as the data has increasing trend at some point and then it becomes constant. This scenario we need to have covariance as 1 and 0 and then model our data using that co-variance. In this scenario we would have a linear part first, and then a constant which would act as a seperate covariance in intervention analysis.

Let's look for stationary dataset 

```{r, message = FALSE, warning = FALSE}
# importing the dataset
CostCap <- data.matrix(train_1[c("加权平均资本成本_zh.wikipedia.org_all-access_spider"),])
dimnames(CostCap)<-NULL
CostCap<-array(CostCap)
plot(CostCap,type='l')
```


The time series appears to have increasing trend with no signs of seasonality 

Let's create a time series object and split the data into test and train. We would use data from 2015-07-01 to 2016-08-31 as train data and data from 2016-09-01 to 2016-12-31 as test data.

Before delving deep into the data the web page let's know about it's story. It's a chinese wikipedia page for `Weighted average cost of capital` which means measure of the cost of capital of a company. Because financing cost is seen as a logical price tag, it was used by many companies as the discount rate for a financing project in the past. It's not a much researched topic and the range is usually in the range of 0-20 views per day but an extremely varying number.

```{r, message = FALSE, warning = FALSE}
time_index <- seq(from = as.POSIXct("2015-07-01"), to = as.POSIXct("2016-12-31"), by = "day")
CostCap_ts <- xts(CostCap, order.by =time_index ,frequency = 365.25)
tsdisplay(CostCap_ts,ylab="WACC daily traffic",xlab="Day")
CostCap_train<-CostCap_ts['2015-07-01/2016-09-30']
CostCap_test<-CostCap_ts['2016-10-01/2016-12-31']
tsdisplay(CostCap_train,ylab="WACC daily traffic",xlab="Day")
tsdisplay(CostCap_test,ylab="WACC daily traffic",xlab="Day")
length(CostCap_test)
```

We can see from ACF and PACF that the dataset isn't dying down fast which means that the dataset isn't stationary and hence we would need differencing. Let's have a look at that as well. Also confirming our assumption that the data isn't stationary

Let's look if we need to do Box Cox transformation to decouple mean and variance
```{r, message = FALSE, warning = FALSE}
BoxCox.lambda(CostCap_train)
```
Yes, we need to perform a transformation with lambda = 0.4686982 which is near to the square root transformation

```{r, message = FALSE, warning = FALSE}
kpss.test(CostCap_train)
```

The p-value = 0.01  
- Since p value is less than 0.05, we reject the NULL hypothesis  
-  The data is `NOT stationary`

Let's transform our data and do on round of non seasonal differencing to check for stationarity.

```{r, message = FALSE, warning = FALSE}
CostCap_train_boxcox<-CostCap_train %>% BoxCox(lambda = BoxCox.lambda(CostCap_train))
CostCap_train_diff <- diff(CostCap_train_boxcox)
kpss.test(CostCap_train_diff)
tsdisplay(CostCap_train_diff)
```
Now the p value is 0.1 and our signal is stationary. We can see that ACF cuts our data at one and PACF Decays exponentially which gives us an idea that this could be MA with lag 1. Let's check it further.

### Naive Forecasts

The data is better now, and the looks stationary. Let's start with naive forecasts first before we move into ARIMA model. 

```{r, message = FALSE, warning = FALSE}
#forecast horizon
h<-92
#naive forecasts
CostCap_train_new<-ts(CostCap_ts['2015-07-01/2016-09-30'])
CostCap_test_new<-ts(CostCap_ts['2016-10-01/2016-12-31'])

#evaluating the models
Model_Mean <- meanf(CostCap_train_new, h) 
Model_Naive <- naive(CostCap_train_new, h) 
Model_Drift <- rwf(CostCap_train_new, h, drift=TRUE)

#Naive forecast
autoplot(CostCap_train_new) +
  autolayer(Model_Mean$mean, series="Mean") +
  autolayer(Model_Naive$mean, series="Naive") +
  autolayer(Model_Drift$mean, series="Drift") +
  ggtitle("Forecasts for daily  Weighted average cost of capital Wikepedia Page") +
  xlab("Days") + ylab("WACC traffic")
```

Let's have a look at the metrics - Out of sample metrics-test


```{r, message = FALSE, warning = FALSE}
accuracy(Model_Mean,CostCap_test)
accuracy(Model_Naive,CostCap_test)
accuracy(Model_Drift,CostCap_test)
```

It can be seen from above metrics that mean is performing the best. Let's have a look at the ARIMA model

### ARIMA Model

```{r, message = FALSE, warning = FALSE}
auto.arima(CostCap_train,seasonal = TRUE,lambda = 'auto')
m1<-Arima(CostCap_train,lambda = 'auto',order=c(0,1,1))
checkresiduals(m1)
```
We were right, with one round of differencing our model shows 0,1 and its a moving average model. Also, We can see that residuals have no autocorrelation and appear stationary. Let's have a look at metrics. Even the Ljung-Box test gives p-value = 0.2536 which shows that residuals are white noise. 

```{r, message = FALSE, warning = FALSE}
autoplot(forecast(m1,h=92))
accuracy(forecast(m1,h=92),CostCap_test)
```
Looking at the metrics it has the lowest Train and a lower RMSE as compared to the other models, and doesn't even overfit, This model performs better than the Naive Forecast.

Let's have a look at EACF now

#### EACF - ARIMA

Let's check if there are better model using the Extended Auto correlation function
```{r, message = FALSE, warning = FALSE}
source("~/Desktop/UChicago/Quarters/03-Quarters/03-31006-TimeSeries/03-Week/eacf.r")
#differencing and box cox transforming the training data 
CostCap_train_new_boxcox<-CostCap_train_new %>% BoxCox(lambda = BoxCox.lambda(CostCap_train_new))
CostCap_train_new_diff <- diff(CostCap_train_new_boxcox)
eacf(CostCap_train_new_diff)
```

Note: We don't wish to complicate the model and hence would keep pmax and qmax to be 2,2
Trying different models from above matrix

p=0,q=1
p=0,q=2
p=1,q=2

```{r, message = FALSE, warning = FALSE}
m2<-Arima(CostCap_train,lambda = 'auto',order=c(0,1,1)) #similiar to previois model
m3<-Arima(CostCap_train,lambda = 'auto',order=c(0,1,2))
m4<-Arima(CostCap_train,lambda = 'auto',order=c(1,1,2))
cbind(m1$aicc,m2$aicc,m3$aicc,m4$aicc)

```

By Aicc values, MODEL-1 is the best. Therefore we would go with auto arima model.

### Exponential Smoothing State Space Model

Looking at ets function as well
```{r, message = FALSE, warning = FALSE}
(Model.ets<-ets(CostCap_train))
autoplot(Model.ets)
```
It shows that the model is Additve Errors with Additive trend and no seasonality but in order to comment on performance we need to check the residuals

```{r, message = FALSE, warning = FALSE}
checkresiduals(Model.ets)
```
The residuals are auto correlated as the p value is 0.0257 from the Ljung-Box, hence we reject this model.

### Cross Validation

tsCV computes the forecast errors obtained by applying forecastfunction to subsets of the time series y using a rolling forecast origin.

Let's use the best model found i.e. ARIMA(2,1,3)
```{r, message = FALSE, warning = FALSE}
h<-92
m7 <- function(x, h){forecast(Arima(x, order=c(0,1,1),lambda = 'auto'), h=h)}
error_1 <- tsCV(CostCap_ts, m7, h=1)
error_2 <- tsCV(CostCap_ts, m7, h=1, window = 24) # Rolling/Sliding Window
```

Let's have look at the errors

```{r, message = FALSE, warning = FALSE}
autoplot(error_1, series = 'Expanding Window') +
  autolayer(error_2, series = 'Rolling Window') 
```

Let's have a look at the values as well

```{r, message = FALSE, warning = FALSE}
print(sqrt(mean(error_1^2, na.rm=TRUE))) 
print(sqrt(mean(error_2^2, na.rm=TRUE))) 
```

Rolling window gives us higher error as compared to expanding window


### TBATS Model

TBATS Stands for

Trigonometric terms for seasonality
Box Cox transformations for heterogeneity
ARMA errors for short term dynamics
Trend (possibly damped)
Seasonal (including multiple and non integer periods)

```{r, message = FALSE, warning = FALSE}
m8 <- tbats(ts(CostCap_train))
m8
```

Since the model isn't seasonal we get BATS model where 
1. The Box-Cox transformation of lambda = 0.958 which is very close to no transformation
2. ARMA error model is {0,0}

```{r, message = FALSE, warning = FALSE}
accuracy(forecast(m8,h=92),CostCap_test)
autoplot(forecast(m8,h=92))
```
RMSE scores isn't comparable to ARIMA(0,1,1) as the model overfits, let's also have a look at the residuals.


Let's also have a look at residuals

```{r, message = FALSE, warning = FALSE}
checkresiduals(m8)
```
Test shows residuals arent autocorrelated as pvalue>0.05 but the errors are way wide and seems like that the model overfits.



Therefore, we would chose AUTO ARIMA which gives us a moving average model with differencing one and square root transformation. The TBATS model doesn't do any differencing but adds a damping factor which overfits the model. We reject the ETS model because it isn't able to capture the autocorrelation between the values.

