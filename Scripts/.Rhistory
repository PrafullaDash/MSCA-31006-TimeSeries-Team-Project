ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
#Naive forecast
autoplot(diff(novela_train)) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
#Naive forecast
autoplot(diff(novela_train)) +
autolayer(Model_SNaive$mean, series="Seasonal naïve")
#Naive forecast
autoplot(diff(novela_train))
Model_Mean
#naive forecasts
time_index <- seq(from = as.POSIXct("2015-07-01"),
to = as.POSIXct("2016-08-31"), by = "day")
Model_Mean <- xts(meanf(novela_train, h),time_index,frequency = 365.25)
length(time_index)
nrow(Model_Mean)
nrow(Model_Naive)
length(Model_Naive)
#naive forecasts
Model_Mean <- meanf(novela_train, h)
Model_Naive <- naive(novela_train, h)
Model_SNaive <- snaive(novela_train,h)
Model_Drift <- rwf(novela_train, h, drift=TRUE)
Model_Mean
#Naive forecast
autoplot(diff(novela_train)) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
#Naive forecast
autoplot(novela_train) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
#Out of sample metrics-test
accuracy(Model_Mean,diff(novela_test))
#Building an ARIMA model
auto.arima(novela_train,seasonal = TRUE,lambda = 'auto')
m1<-Arima(novela_train,lambda = 'auto',order=c(0,1,2))
checkresiduals(m1)
forecast(m1,h=90)
autoplot(forecast(m1,h=90))
#checking the forecast
accuracy(forecast(m1,h=90),diff(novela_test))
#Building an ARIMA model
auto.arima(novela_train,seasonal = TRUE,lambda = 'auto')
m1<-Arima(novela_train,lambda = 'auto',order=c(2,1,3),include.drift = TRUE)
checkresiduals(m1)
forecast(m1,h=90)
autoplot(forecast(m1,h=90))
#naive forecasts
Model_Mean <- meanf(novela_train, h)
Model_Mean
time_index <- seq(from = as.POSIXct("2015-07-01"),
to = as.POSIXct("2016-08-31"), by = "day")
length(time_index)
#forecast horizon
h<-428
#naive forecasts
Model_Mean <- meanf(novela_train, h)
Model_Naive <- naive(novela_train, h)
Model_SNaive <- snaive(novela_train,h)
Model_Drift <- rwf(novela_train, h, drift=TRUE)
Model_Mean
#Naive forecast
autoplot(novela_train) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
Model_Mean<-xts(Model_Mean,order.by =time_index ,frequency = 365.25)
Model_Mean
Model_Mean$mean
Model_Mean<-xts(Model_Mean$mean,order.by =time_index ,frequency = 365.25)
#Naive forecast
autoplot(novela_train)
Model_Mean$mean
Model_Mean
#Naive forecast
autoplot(novela_train)
Model_Mean<-xts(Model_Mean$mean,order.by =time_index ,frequency = 365.25)
Model_Mean
#Naive forecast
autoplot(novela_train)+
autolayer(Model_Mean$mean, series="Mean")
library('xts')
library('zoo')
#Naive forecast
autoplot(novela_train)+
autolayer(Model_Mean$mean, series="Mean")
#naive forecasts
Model_Mean <- meanf(novela_train, h)
#Naive forecast
autoplot(novela_train)+
autolayer(Model_Mean$mean, series="Mean")
#Naive forecast
autoplot(ts(novela_train))+
autolayer(Model_Mean$mean, series="Mean")
#splitting data into test and train
novela_train<-ts(novela_ts['2015-07-01/2016-08-31'])
novela_test<-ts(novela_ts['2016-09-01/2016-12-31'])
tsdisplay(novela_train)
tsdisplay(novela_test)
#requirement for transformation
BoxCox.lambda(novela_train)
autoplot(novela_train)
ggAcf(novela_train)
ggPacf(novela_train)
#testing stationary
kpss.test(novela_train)
#Therefore, it is non stationary and we need to difference it
novela_train_diff <- diff(novela_train)
kpss.test(novela_train_diff)
tsdisplay(novela_train_diff)
#forecast horizon
h<-428
#naive forecasts
Model_Mean <- meanf(novela_train, h)
Model_Naive <- naive(novela_train, h)
Model_SNaive <- snaive(novela_train,h)
Model_Drift <- rwf(novela_train, h, drift=TRUE)
#Naive forecast
autoplot(novela_train)+
autolayer(Model_Mean$mean, series="Mean")
#naive forecasts
Model_Mean <- meanf(novela_train, h)
Model_Naive <- naive(novela_train, h)
Model_SNaive <- snaive(novela_train,h)
Model_Drift <- rwf(novela_train, h, drift=TRUE)
#Naive forecast
autoplot(diff(Google_train_diff)) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
#Naive forecast
autoplot(diff(novela_train)) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
Model_SNaive <- snaive(novela_train,h)
#Naive forecast
autoplot(diff(novela_train)) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
#Out of sample metrics-test
accuracy(Model_Mean,diff(novela_test))
#Out of sample metrics-test
accuracy(Model_Mean,novela_test)
Model_Mean
novela_test
#splitting data into test and train
novela_train<-novela_ts['2015-07-01/2016-08-31']
novela_test<-novela_ts['2016-09-01/2016-12-31']
tsdisplay(novela_train)
tsdisplay(novela_test)
#requirement for transformation
BoxCox.lambda(novela_train)
autoplot(novela_train)
ggAcf(novela_train)
ggPacf(novela_train)
#testing stationary
kpss.test(novela_train)
#Therefore, it is non stationary and we need to difference it
novela_train_diff <- diff(novela_train)
kpss.test(novela_train_diff)
tsdisplay(novela_train_diff)
#forecast horizon
h<-428
time_index <- seq(from = as.POSIXct("2015-07-01"),
to = as.POSIXct("2016-08-31"), by = "day")
#naive forecasts
novela_train_new<-ts(novela_ts['2015-07-01/2016-08-31'])
novela_test_new<-ts(novela_ts['2016-09-01/2016-12-31'])
Model_Mean <- meanf(novela_train_new, h)
Model_Naive <- naive(novela_train_new, h)
Model_SNaive <- snaive(novela_train_new,h)
Model_Drift <- rwf(novela_train_new, h, drift=TRUE)
#Naive forecast
autoplot(diff(novela_train_new)) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
#Out of sample metrics-test
accuracy(Model_Mean,novela_test)
accuracy(Model_Naive,diff(novela_test))
accuracy(Model_SNaive,diff(novela_test))
accuracy(Model_Drift,diff(novela_test))
#Building an ARIMA model
auto.arima(novela_train,seasonal = TRUE,lambda = 'auto')
m1<-Arima(novela_train,lambda = 'auto',order=c(2,1,3),include.drift = TRUE)
checkresiduals(m1)
forecast(m1,h=90)
#checking the forecast
accuracy(forecast(m1,h=90),novela_test)
#Building an ARIMA model
auto.arima(novela_train,seasonal = TRUE,lambda = 'auto')
m1<-Arima(novela_train,lambda = 'auto',order=c(2,1,3),include.drift = TRUE)
checkresiduals(m1)
forecast(m1,h=428)
autoplot(forecast(m1,h=428))
#checking the forecast
accuracy(forecast(m1,h=428),novela_test)
#load libraries
# visualization
library('ggplot2')
# data manipulation
library('dplyr')
library('readr')
library('imputeTS')
#time series
library('fpp')
library('forecast')
library('xts')
library('zoo')
#importing the data and considering just one page
train_1 <- read.csv("~/Desktop/UChicago/Quarters/03-Quarters/Data/TS/web-traffic-time-series-forecasting/train_1.csv", header = TRUE, row.names = 1,sep = ",",skip =0)
#Tech <- data.frame(train_1[c("Google_zh.wikipedia.org_all-access_spider","Apple_II_zh.wikipedia.org_all-access_spider","Facebook_zh.wikipedia.org_all-access_spider","YouTube_zh.wikipedia.org_all-access_spider","Android_zh.wikipedia.org_all-access_spider"),])
novela <- data.matrix(train_1[c("ASCII_zh.wikipedia.org_all-access_spider"),])
#dropping the colnames and creating a time series object
dimnames(novela)<-NULL
novela<-array(novela)
head(novela)
length(novela)
plot(novela,type='l')
#creating a time series object
#novela_ts<-ts(novela,frequency = 365.25,start = c(2015, 7, 1))
time_index <- seq(from = as.POSIXct("2015-07-01"),
to = as.POSIXct("2016-12-31"), by = "day")
novela_ts <- xts(novela, order.by =time_index ,frequency = 365.25)
novela_ts
autoplot(novela_ts,ylab="1984 daily traffic",xlab="Day")
#splitting data into test and train
novela_train<-novela_ts['2015-07-01/2016-08-31']
novela_test<-novela_ts['2016-09-01/2016-12-31']
tsdisplay(novela_train)
tsdisplay(novela_test)
#naive forecasts
novela_train_new<-ts(novela_ts['2015-07-01/2016-08-31'])
novela_test_new<-ts(novela_ts['2016-09-01/2016-12-31'])
#splitting data into test and train
novela_train<-novela_ts['2015-07-01/2016-08-31']
novela_test<-novela_ts['2016-09-01/2016-12-31']
tsdisplay(novela_train)
tsdisplay(novela_test)
#creating a time series object
#novela_ts<-ts(novela,frequency = 365.25,start = c(2015, 7, 1))
time_index <- seq(from = as.POSIXct("2015-07-01"),
to = as.POSIXct("2016-12-31"), by = "day")
novela_ts <- xts(novela, order.by =time_index ,frequency = 365)
novela_ts
autoplot(novela_ts,ylab="1984 daily traffic",xlab="Day")
#splitting data into test and train
novela_train<-novela_ts['2015-07-01/2016-08-31']
novela_test<-novela_ts['2016-09-01/2016-12-31']
tsdisplay(novela_train)
tsdisplay(novela_test)
novela_ts
#creating a time series object
#novela_ts<-ts(novela,frequency = 365.25,start = c(2015, 7, 1))
time_index <- seq(from = as.POSIXct("2015-07-01"),
to = as.POSIXct("2016-12-31"), by = "day")
novela_ts <- xts(novela, order.by =time_index ,frequency = 365.25)
novela_ts
autoplot(novela_ts,ylab="1984 daily traffic",xlab="Day")
#splitting data into test and train
novela_train<-xts(novela_ts['2015-07-01/2016-08-31'])
novela_test<-novela_ts['2016-09-01/2016-12-31']
tsdisplay(novela_train)
tsdisplay(novela_test)
#load libraries
# visualization
library('ggplot2')
# data manipulation
library('dplyr')
library('readr')
library('imputeTS')
#time series
library('fpp')
library('forecast')
library('xts')
library('zoo')
#importing the data and considering just one page
train_1 <- read.csv("~/Desktop/UChicago/Quarters/03-Quarters/Data/TS/web-traffic-time-series-forecasting/train_1.csv", header = TRUE, row.names = 1,sep = ",",skip =0)
#Tech <- data.frame(train_1[c("Google_zh.wikipedia.org_all-access_spider","Apple_II_zh.wikipedia.org_all-access_spider","Facebook_zh.wikipedia.org_all-access_spider","YouTube_zh.wikipedia.org_all-access_spider","Android_zh.wikipedia.org_all-access_spider"),])
novela <- data.matrix(train_1[c("ASCII_zh.wikipedia.org_all-access_spider"),])
#dropping the colnames and creating a time series object
dimnames(novela)<-NULL
novela<-array(novela)
head(novela)
length(novela)
plot(novela,type='l')
#creating a time series object
#novela_ts<-ts(novela,frequency = 365.25,start = c(2015, 7, 1))
time_index <- seq(from = as.POSIXct("2015-07-01"),
to = as.POSIXct("2016-12-31"), by = "day")
novela_ts <- xts(novela, order.by =time_index ,frequency = 365.25)
novela_ts
autoplot(novela_ts,ylab="1984 daily traffic",xlab="Day")
#splitting data into test and train
novela_train<-novela_ts['2015-07-01/2016-08-31']
novela_test<-novela_ts['2016-09-01/2016-12-31']
tsdisplay(novela_train)
tsdisplay(novela_test)
#requirement for transformation
BoxCox.lambda(novela_train)
autoplot(novela_train)
ggAcf(novela_train)
ggPacf(novela_train)
#testing stationary
kpss.test(novela_train)
#Therefore, it is non stationary and we need to difference it
novela_train_diff <- diff(novela_train)
kpss.test(novela_train_diff)
tsdisplay(novela_train_diff)
#forecast horizon
h<-428
#naive forecasts
novela_train_new<-ts(novela_ts['2015-07-01/2016-08-31'])
novela_test_new<-ts(novela_ts['2016-09-01/2016-12-31'])
Model_Mean <- meanf(novela_train_new, h)
Model_Naive <- naive(novela_train_new, h)
Model_SNaive <- snaive(novela_train_new,h)
Model_Drift <- rwf(novela_train_new, h, drift=TRUE)
#Naive forecast
autoplot(diff(novela_train_new)) +
autolayer(Model_SNaive$mean, series="Seasonal naïve") +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naïve") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily Google Wikepedia Page") +
xlab("Days") + ylab("Google traffic")
#Out of sample metrics-test
accuracy(Model_Mean,novela_test)
accuracy(Model_Naive,diff(novela_test))
accuracy(Model_SNaive,diff(novela_test))
accuracy(Model_Drift,diff(novela_test))
#Building an ARIMA model
auto.arima(novela_train,seasonal = TRUE,lambda = 'auto')
m1<-Arima(novela_train,lambda = 'auto',order=c(2,1,3),include.drift = TRUE)
checkresiduals(m1)
forecast(m1,h=428)
autoplot(forecast(m1,h=428))
#checking the forecast
accuracy(forecast(m1,h=428),novela_test)
# visualization
library('ggplot2')
# data manipulation
library('dplyr')
library('readr')
library('imputeTS')
#time series
library('fpp')
library('forecast')
library('xts')
library('zoo')
# importing the dataset
train_1 <- read.csv("~/Desktop/UChicago/Quarters/03-Quarters/Data/TS/web-traffic-time-series-forecasting/train_1.csv", header = TRUE, row.names = 1,sep = ",",skip =0)
View(train_1)
# importing the dataset
ASCII <- data.matrix(train_1[c("ASCII_zh.wikipedia.org_all-access_spider"),])
dimnames(ASCII)<-NULL
ASCII<-array(ASCII)
plot(ASCII,type='l')
time_index <- seq(from = as.POSIXct("2015-07-01"), to = as.POSIXct("2016-12-31"), by = "day")
ASCII_ts <- xts(ASCII, order.by =time_index ,frequency = 365.25)
tsdisplay(ASCII_ts,ylab="ASCII daily traffic",xlab="Day")
ASCII_train<-ASCII_ts['2015-07-01/2016-09-30']
ASCII_test<-ASCII_ts['2016-10-01/2016-12-31']
tsdisplay(ASCII_train,ylab="ASCII daily traffic",xlab="Day")
tsdisplay(ASCII_test,ylab="ASCII daily traffic",xlab="Day")
length(ASCII_test)
BoxCox.lambda(ASCII_train)
kpss.test(ASCII_train)
ASCII_train_boxcox<-ASCII_train %>% BoxCox(lambda = BoxCox.lambda(ASCII_train))
ASCII_train_diff <- diff(ASCII_train_boxcox)
kpss.test(ASCII_train_diff)
tsdisplay(ASCII_train_diff)
#forecast horizon
h<-92
#naive forecasts
ASCII_train_new<-ts(ASCII_ts['2015-07-01/2016-09-30'])
ASCII_test_new<-ts(ASCII_ts['2016-10-01/2016-12-31'])
#evaluating the models
Model_Mean <- meanf(ASCII_train_new, h)
Model_Naive <- naive(ASCII_train_new, h)
Model_Drift <- rwf(ASCII_train_new, h, drift=TRUE)
#Naive forecast
autoplot(ASCII_train_new) +
autolayer(Model_Mean$mean, series="Mean") +
autolayer(Model_Naive$mean, series="Naive") +
autolayer(Model_Drift$mean, series="Drift") +
ggtitle("Forecasts for daily ASCII Wikepedia Page") +
xlab("Days") + ylab("ASCII traffic")
accuracy(Model_Mean,ASCII_test)
accuracy(Model_Naive,ASCII_test)
accuracy(Model_Drift,ASCII_test)
auto.arima(ASCII_train,seasonal = TRUE,lambda = 'auto')
m1<-Arima(ASCII_train,lambda = 'auto',order=c(2,1,3),include.drift = TRUE)
checkresiduals(m1)
autoplot(forecast(m1,h=92))
accuracy(forecast(m1,h=92),ASCII_test)
source("~/Desktop/UChicago/Quarters/03-Quarters/03-31006-TimeSeries/03-Week/eacf.r")
#differencing and box cox transforming the training data
ASCII_train_new_boxcox<-ASCII_train_new %>% BoxCox(lambda = BoxCox.lambda(ASCII_train_new))
ASCII_train_new_diff <- diff(ASCII_train_new_boxcox)
eacf(ASCII_train_new_diff)
m2<-Arima(ASCII_train,lambda = 'auto',order=c(0,1,1),include.drift = TRUE)
m3<-Arima(ASCII_train,lambda = 'auto',order=c(0,1,2),include.drift = TRUE)
m4<-Arima(ASCII_train,lambda = 'auto',order=c(1,1,2),include.drift = TRUE)
m5<-Arima(ASCII_train,lambda = 'auto',order=c(2,1,1),include.drift = TRUE)
m6<-Arima(ASCII_train,lambda = 'auto',order=c(2,1,2),include.drift = TRUE)
cbind(m1$aicc,m2$aicc,m3$aicc,m4$aicc,m5$aicc,m6$aicc)
m7 <- function(x, h){forecast(Arima(x, order=c(2,1,3),lambda = 'auto',include.drift=TRUE), h=h)}
error_1 <- tsCV(ASCII_train, m7, h=1)
error_2 <- tsCV(ASCII_train, m7, h=1, window = 12) # Rolling/Sliding Window
autoplot(error_1, series = 'Expanding Window') +
autolayer(error_2, series = 'Rolling Window')
print(sqrt(mean(error_1^2, na.rm=TRUE)))
print(sqrt(mean(error_2^2, na.rm=TRUE)))
m_hw <- hw(ASCII_train,h=92)
m_hw <- holt(ASCII_train,h=92)
plot(m_hw)
m_hw <- holt(ASCII_train,h=92)
autoplot(m_hw)
m_hw
accuracy(m_hw)
m_hw <- holt(ASCII_train,h=92)
autoplot(m_hw)
m_hw_damp <- holt(ASCII_train,h=92,damped = TRUE)
autoplot(m_hw_damp)
accuracy(m_hw)
accuracy(m_hw_damp)
checkresiduals(m_hw)
checkresiduals(m_hw_damp)
m_hw <- holt(ASCII_train,h=92)
autoplot(m_hw)
m_hw_damp <- holt(ASCII_train,h=92,damped = TRUE)
autoplot(m_hw_damp)
accuracy(m_hw)
accuracy(m_hw_damp)
checkresiduals(m_hw)
checkresiduals(m_hw_damp)
library('prophet')
install.packages("prophet", type="source")
library('prophet')
# visualization
library('ggplot2')
# data manipulation
library('dplyr')
library('readr')
library('imputeTS')
#time series
library('fpp')
library('forecast')
library('xts')
library('zoo')
library('prophet)
# visualization
library('ggplot2')
# data manipulation
library('dplyr')
library('readr')
library('imputeTS')
#time series
library('fpp')
library('forecast')
library('xts')
library('zoo')
library('prophet')
# importing the dataset
train_1 <- read.csv("~/Desktop/UChicago/Quarters/03-Quarters/Data/TS/web-traffic-time-series-forecasting/train_1.csv", header = TRUE, row.names = 1,sep = ",",skip =0)
# visualization
library('ggplot2')
# data manipulation
library('dplyr')
library('readr')
library('imputeTS')
#time series
library('fpp')
library('forecast')
library('xts')
library('zoo')
library('prophet')
# importing the dataset
train_1 <- read.csv("~/Desktop/UChicago/Quarters/03-Quarters/Data/TS/web-traffic-time-series-forecasting/train_1.csv", header = TRUE, row.names = 1,sep = ",",skip =0)
