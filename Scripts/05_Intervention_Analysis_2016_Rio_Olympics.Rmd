---
title: "Intervention_Analysis_2016_Rio_Olympics"
author: "Prafulla Ranjan Dash"
output: html_document
---

#### Here, we try and apply Intervention Analysis for the number of views time-series for the *2016_Summer_Olympics_en* webpage.

```{r workdirimp, echo=TRUE, message=FALSE, warning=FALSE}
# basic imports
library("fpp", lib.loc="C:/Users/HP/Anaconda3/envs/rstudio/lib/R/library")
library("ggplot2", lib.loc="C:/Users/HP/Anaconda3/envs/rstudio/lib/R/library")
library("stats", lib.loc="C:/Users/HP/Anaconda3/envs/rstudio/lib/R/library")
library("zoo", lib.loc="C:/Users/HP/Anaconda3/envs/rstudio/lib/R/library")
library("TSA", lib.loc="C:/Users/HP/Anaconda3/envs/rstudio/lib/R/library")
```

#### Importing the data

```{r dataimport, echo=TRUE, message=FALSE, warning=FALSE}
setwd("D:/USMS/UChicago/STUDIES/Autumn 2020/MSCA 31006 1 Time Series Analysis and Forecasting/Final Project/Data")
page_2016_Rio_Olympics_views <- data.matrix(read.csv("train_1.csv", header = TRUE, row.names = 1,sep = ",",nrows = 1,skip = 12186))
```

#### Preparing data to make it ready for time-series

```{r dataprepare, echo=TRUE, message=FALSE, warning=FALSE}
# removing column names
dimnames(page_2016_Rio_Olympics_views) <- NULL

# converting to array
page_2016_Rio_Olympics_views <- array(page_2016_Rio_Olympics_views)

# checking the values
head(page_2016_Rio_Olympics_views)

# no. of ovservations
length(page_2016_Rio_Olympics_views)
```

Looking at the entire data - 

```{r view, echo=TRUE, message=FALSE, warning=FALSE}
plot(page_2016_Rio_Olympics_views,type = 'l')
```

Looking at this dataset, we can see there are few time periods where there seems to be a clear intervention at t=175. So, we will take a sub-section of this data for our analysis.

```{r split, echo=TRUE, message=FALSE, warning=FALSE}
#page_2016_Rio_Olympics_views <- page_2016_Rio_Olympics_views[200:400]
page_2016_Rio_Olympics_views <- page_2016_Rio_Olympics_views[100:400]

# converting data to ts
page_2016_Rio_Olympics_views <- ts(page_2016_Rio_Olympics_views,frequency = 1)

# plotting the time-series
autoplot(page_2016_Rio_Olympics_views)
```

So, here we can clearly see some sort of **intervention** that took place at around the time-period of 75 days for this chunk of data (175 days overall). There is a clear **upward trend** after the time-period of 75 days. 

Now, since the intervention is at time-period 75, we will divide our data into data-points prior to time-period 75 and post time-period 75. We will do this so that we can find the **underlying process** that is present in our observations, which are a **mix of the underlying process and the intervention**.

Now, looking at the dates, this intervention took place on **23rd Dec 2015**. And there was an article which was published in the *The Guardian* newspaper on **8th Dec 2015** that there have been widespread human right violations in Rio de Janeiro because of the evictions taking place for creating the infrastructure needed for conducting the Olympics. So, we believe this report of **Human rights violation related to Summer Olympics 2016** could have generated the curiosity amongst the readers which resulted in the increase in the upward trend for the number of views for this Wikipedia page.

Also, we will split our data to train and test so that we can see how our model performs on new data.

```{r split2, echo=TRUE, message=FALSE, warning=FALSE}
# splitting to prior and post intervention
page_2016_Rio_Olympics_views_prior <- page_2016_Rio_Olympics_views[1:75]
page_2016_Rio_Olympics_views_post <- page_2016_Rio_Olympics_views[76:300]

# converting to time-series
page_2016_Rio_Olympics_views_prior <- ts(page_2016_Rio_Olympics_views_prior)
page_2016_Rio_Olympics_views_post <- ts(page_2016_Rio_Olympics_views_post)
```

Now, let's see how our prior intervention data looks

```{r prior, echo=TRUE, message=FALSE, warning=FALSE}
autoplot(page_2016_Rio_Olympics_views_prior)
```


Looking at the data - 

1. There is no visible non-seasonal trend.  
2. There might be some seasonality where some patterns are repeating themselves.
3. The variance seems unstable, might require some Box-Cox transformation.

Applying Box-Cox - 

```{r boxcox, echo=TRUE, message=FALSE, warning=FALSE}
BoxCox.lambda(page_2016_Rio_Olympics_views_prior)

page_2016_Rio_Olympics_views_prior_transformed <- BoxCox(page_2016_Rio_Olympics_views_prior,lambda = -0.6576963)

autoplot(page_2016_Rio_Olympics_views_prior_transformed)
```

The variance now seems to have stabilized a bit. Now, let's look at the ACF and PACF for this signal.

```{r acf, echo=TRUE, message=FALSE, warning=FALSE}
tsdisplay(page_2016_Rio_Olympics_views_prior_transformed)
```

We can see from the ACF that our data is **not stationary** as there are significant spikes along with a sinusoidal component. Let's apply one order of non-seasonal differencing.

```{r diff, echo=TRUE, message=FALSE, warning=FALSE}
# differencing
page_2016_Rio_Olympics_views_prior_transformed_diff <- diff(page_2016_Rio_Olympics_views_prior_transformed)

# visulalizing
tsdisplay(page_2016_Rio_Olympics_views_prior_transformed_diff)
```

one order of seasonal differencing


```{r sdiff, echo=TRUE, message=FALSE, warning=FALSE}
# differencing
page_2016_Rio_Olympics_views_prior_transformed_diff2 <- diff(page_2016_Rio_Olympics_views_prior_transformed_diff,lag = 7)

# visulalizing
tsdisplay(page_2016_Rio_Olympics_views_prior_transformed_diff2)
```


We see that the data still does not appear stationary. There is 1 very significant spike at lag 7.

Let's allow auto.arima to decide the best model for this data.

```{r m_prior, echo=TRUE, message=FALSE, warning=FALSE}
# applying auto.arima
m_prior <- auto.arima(page_2016_Rio_Olympics_views_prior,lambda = 'auto')

# checking the model
summary(m_prior)

# diagnozing model - residuals
checkresiduals(m_prior)
```

So, we see that the underlying process here is ARIMA(5,1,2). The residuals here seem to be white noise as there are no significant spikes for ACF of residuals. Looking at the residuals time-series, there is one outlier.

```{r model, echo=TRUE, message=FALSE, warning=FALSE}
# forecasting
m_prior_forecast <- forecast(m_prior,h = 225)

# plotting the forecasts
plot(m_prior_forecast)
```

This is how the time-series would have looked if there was no intervention. Now, let's try and analyze what could be the transfer function that represents the intervention correctly.

For this, we can just subtract the underlying process from the observations. That will give us the intervention. We suspect that the intervention can be described as AR(1) process.

```{r model11, echo=TRUE, message=FALSE, warning=FALSE}
# taking the point of intervention
IHRV <- 1*(seq(page_2016_Rio_Olympics_views)==75)

# applying the model
page_views_mPulse <- arimax(log(page_2016_Rio_Olympics_views), order = c(5,1,2),
                     xtransf = data.frame(IHRV,IHRV),
                     transfer = list(c(0,0),c(1,0)),
                     xreg = data.frame(Dec96=1*(seq(page_2016_Rio_Olympics_views)==12),
                                       Jan97=1*(seq(page_2016_Rio_Olympics_views)==13),
                                       Dec02=1*(seq(page_2016_Rio_Olympics_views)==84)),
                     method = 'ML'
                     )
# viewing the model
page_views_mPulse
```

Here we applied the Regression with ARMA error for capturing the underlying process and the intervention. We assumed that the transfer function that can represent the intervention is an AR(1) function and we used the coefficients got from here to plot the intervention below.

```{r viewintervention, echo=TRUE, message=FALSE, warning=FALSE}
plot(ts(IHRV*(0.0883)+filter(IHRV,filter = 1.0118, method = 'recursive', sides = 1)*(0.0883)),
     type = 'h',ylab = 'Pulse effects'
     )
```

The above may be the intervention that took place.

Now, let's check the residuals of this model, how they performed.

```{r modeldiag, echo=TRUE, message=FALSE, warning=FALSE}
checkresiduals(page_views_mPulse)
```

From the residuals ACF we can see that the residuals are not white noise and the model was not able to capture all the systematic patterns present in the data. We have to keep improving our model further.
